{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering examples\n",
    "\n",
    "This notebook is for demonstrating how to use the fuzzy water clustering package with scikit-learn.\n",
    "\n",
    "The basic idea is that we create some scikit-learn compatible clustering estimators and a group of scoring functions. They can then be thrown about using scikit-learn for the following important purposes.\n",
    "\n",
    "* [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) : Chain together pre-processing, clustering and scoring steps into one object\n",
    "* [Model evaluation](https://scikit-learn.org/stable/model_selection.html#model-selection) : Cross validation of models to find optimum fitting parameters as scored against various scoring functions\n",
    "\n",
    "We aim to build clustering estimators, store them in ./Models. Additional capability we are looking to add:\n",
    "\n",
    "* Ensemble methods (run many times to assess stability)\n",
    "* I/O helper functions (read csv, netcdf, whatever)\n",
    "* Suite of scoring metrics (fuzzy silouette score, partition coefficient, etc)\n",
    "* Default, pretrained models (one for each common dataset, like OLCI, OC-CCI etc)\n",
    "\n",
    "in a way that is generalised to work on all models. Must be a scikit-learn compatible object and must not (!) reinvent wheels here\n",
    "\n",
    "Note, best practice is to install dependencies concurrently in case they have not loaded correctly from pip install of package:\n",
    "* conda install notebook numpy xarray dask scikit-learn scikit-fuzzy hvplot holoviews datashader netcdf4 h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the whole package\n",
    "import fuzzy_water_clustering as fwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for serialization of arbitrary Python objects\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# scikit-learn objects for many things\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score #, pairwise_distances, make_scorer, \n",
    "# from sklearn.metrics.cluster import calinski_harabasz_score, davies_bouldin_score, silhouette_samples, silhouette_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for data handling and visualization\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, labels = make_blobs(n_samples=2000, n_features=11)\n",
    "dfb = pd.DataFrame(blobs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 using cluster estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a instance of cmeans model, with number of clusters c\n",
    "cmeans = fwc.CmeansModel(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit against the dataset, this generates a cluster set\n",
    "cmeans.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cntr = pd.DataFrame(cmeans.cluster_centers_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.T[labels == 0].T.hvplot(kind='line',datashade=True, cmap='Reds', label='Cluster 1') *\\\n",
    "dfb.T[labels == 1].T.hvplot(kind='line',datashade=True, cmap='Blues', label='Cluster 2') *\\\n",
    "dfb.T[labels == 2].T.hvplot(kind='line',datashade=True, cmap='Greens', label='Cluster 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(lambda a,b:a*b, [dfb.T[labels == i].mean().T.hvplot(c=c) for (i,c) in zip(range(3),['red','blue','green'])]).opts(title='cluster centres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the classified data\n",
    "df_cntr.hvplot(title='cluster centres found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Performance metrics\n",
    "\n",
    "if an algorithm comes with its own scoring metric that will be class specific. However, we can get some metrics that are applicable across all methods. \n",
    "\n",
    "Scikit-learn has a [suite of metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "Probably the best way is to make a scorer object from a scoring function with `sklearn.metrics.make_scorer` which can be placed at after clustering in a pipeline which is then fed to a GridSearch like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = fwc.cluster_scoring.silhouette_samples(blobs, cmeans.labels_)\n",
    "hv.Bars(\n",
    "    np.hstack([sorted(ss[cmeans.labels_==i]) for i in range(3)]),\n",
    ").opts(title=f\"silhouette_score = {np.round(silhouette_score(blobs, cmeans.labels_), 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Pipelines: chaining transforms and estimators together\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/compose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy to make a pipeline with whatever pieces you want\n",
    "pl = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(),\n",
    "    fwc.CmeansModel(n_clusters=3)     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.x inverse_transforms to get final results in original space\n",
    "\n",
    "Each transform step in a pipeline has an inverse_transform method. Enabling us to reverse the trasform so that final results can be projected into the original space.\n",
    "\n",
    "In this example, scaling of the data and principal component analysis are applied before clustering. Therefore the clusters are defined in a transformed space. Using inverse_transforms we regain the orginal x,y space the data came in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cntr = pd.DataFrame(\n",
    "    pl['standardscaler'].inverse_transform(\n",
    "        pl['pca'].inverse_transform(\n",
    "            pl['cmeansmodel'].cluster_centers_\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.hvplot(kind='line', datashade=True, width=1000) * \\\n",
    "df_cntr.T.hvplot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 GridSearchCV: Exhaustive search to find best fitting parameters\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem we have is that the clustering algorithms performance depends on the fitting parameters (hyper paramters). These are not learnt, rather they determine the behaviour of the algorithm when finding the learnt parameters. GridSearchCV helps by exhaustively fitting the model to the data for every combination of fitting paramters supplied to it. Furthermore, it fits and scores the model to 5 subsets of the data (by default) so that the variance of the score can be measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dict of parameters to go through all combinations of values.\n",
    "# double underscore joins the step name to its parameter\n",
    "param_grid = {\n",
    "    'cmeansmodel__c': [2,4,6,8,10],\n",
    "    'cmeansmodel__m':[1.5,2.0,2.5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scoring metrics you want to evaluate\n",
    "scoring = {\n",
    "    'XB': fwc.cluster_scoring.xie_beni,\n",
    "    'SIL': fwc.cluster_scoring.hard_silouette,\n",
    "    'FPC': fwc.cluster_scoring.fuzzy_partition_coef,\n",
    "    'DB': fwc.cluster_scoring.davies_bouldin,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid search cross validation object\n",
    "gs = GridSearchCV(\n",
    "    pl,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='XB',\n",
    "    n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting syntax is just like before. Except now it tries all combinations of parameters,\n",
    "# scores and refits witht he best according to the chosen refit metric.\n",
    "gs.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas.DataFrame that contains the results of the fitting\n",
    "dfgs = pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.x plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce(lambda a,b:a*b,[dfgs.hvplot(\n",
    "        col='param_cmeansmodel__m',\n",
    "        row=None,\n",
    "        kind='scatter',\n",
    "        x='param_cmeansmodel__c',\n",
    "        y=f'mean_test_{x}',\n",
    "        label=f'{x}',\n",
    "        legend=False,\n",
    "        ylim=[-10,2.5],\n",
    "        ylabel=\"score\",\n",
    "        xlabel=r\"# clusters\",\n",
    ") for x in scoring.keys()]).opts(title=\"3 blobs, 2000 points\", show_legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 serializing models\n",
    "\n",
    "If we like our results and we want to save them. There exist a few choices;\n",
    "\n",
    "* pickle / joblib : stores arbitrary python objects in instruction orientated file. Insecure on loading, not cross platform nor cross version supported. Only open trusted files and best used short term.\n",
    "* custom serialization : pipeline parameters can be stored to netcdf using utils/serialize_models. But it also isn't cross platform nor cross version supported at present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 pickle/joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a model as a pickle file\n",
    "pickle.dump(\n",
    "    pl,\n",
    "    open(\n",
    "        \"practice_fitted_pipeline.p\",\n",
    "        \"wb\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Application to OLCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THREDDS pathway for Plymouth Sound\n",
    "GEO_DAILY1km_THREDDS_string = 'http://rsg.pml.ac.uk/thredds/dodsC/SENTINEL3A_OLCI-G13_300m_02-1d'\n",
    "# GEO_DAILY1km_THREDDS_string = '../../data/OLCI_RRS_spectral_library_968349_pixels_from_6000_random_G25_300m_files.nc'\n",
    "ds = xr.open_dataset(GEO_DAILY1km_THREDDS_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().pivot_table(index='pixel',columns='wavelength',values='Rrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cntr = pd.DataFrame(\n",
    "    pl['standardscaler'].inverse_transform(\n",
    "        pl['pca'].inverse_transform(\n",
    "            pl['cmeansmodel'].cntr_\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfb.hvplot(kind='scatter',x='x',y='y', alpha=0.5, s=1) * \\\n",
    "df_cntr.reset_index().hvplot(kind='scatter',x='x',y='y',c='index', cmap='rainbow', s=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
